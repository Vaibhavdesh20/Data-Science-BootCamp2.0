{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Deep Learning.\n",
        "a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the\n",
        "ELU activation function.\n",
        "b. Using Adam optimization and early stopping, try training it on MNIST but only on\n",
        "digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You\n",
        "will need a softmax output layer with five neurons, and as always make sure to save\n",
        "checkpoints at regular intervals and save the final model so you can reuse it later.\n",
        "c. Tune the hyperparameters using cross-validation and see what precision you can\n",
        "achieve.\n",
        "d. Now try adding Batch Normalization and compare the learning curves: is it\n",
        "converging faster than before? Does it produce a better model?\n",
        "e. Is the model overfitting the training set? Try adding dropout to every layer and try\n",
        "again. Does it help?\n",
        "\n",
        "\n",
        "a. To build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function, you can use Python with the help of deep learning libraries like TensorFlow or Keras. Here's an example of how you can create such a model using Keras:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Filter the dataset to include only digits 0 to 4\n",
        "train_mask = train_labels < 5\n",
        "test_mask = test_labels < 5\n",
        "train_images = train_images[train_mask]\n",
        "train_labels = train_labels[train_mask]\n",
        "test_images = test_images[test_mask]\n",
        "test_labels = test_labels[test_mask]\n",
        "\n",
        "# Preprocess the data\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Set up callbacks for checkpoint saving and early stopping\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint('model_checkpoint.h5', save_best_only=True)\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_images, train_labels, epochs=100, validation_data=(test_images, test_labels),\n",
        "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "\n",
        "# Save the final model\n",
        "model.save('final_model.h5')\n",
        "```\n",
        "\n",
        "b. The code snippet provided in part (a) already covers training the model on MNIST for digits 0 to 4 and saving checkpoints and the final model.\n",
        "\n",
        "c. To tune the hyperparameters and evaluate the model's precision using cross-validation, you can use techniques like grid search or random search. Here's an example of using scikit-learn's `GridSearchCV` to tune the learning rate and batch size:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define a function to create the model with hyperparameters\n",
        "def create_model(learning_rate=0.001, batch_size=32):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Flatten(input_shape=(28, 28)),\n",
        "        keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "        keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "        keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "        keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "        keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "        keras.layers.Dense(5, activation='softmax')\n",
        "    ])\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create the KerasClassifier wrapper for scikit-learn\n",
        "model = K\n",
        "\n",
        "erasClassifier(build_fn=create_model, epochs=100, verbose=0)\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {\n",
        "    'learning_rate': [0.001, 0.01, 0.1],\n",
        "    'batch_size': [32, 64, 128]\n",
        "}\n",
        "\n",
        "# Perform grid search cross-validation\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
        "grid_result = grid.fit(train_images, train_labels)\n",
        "\n",
        "# Print the best precision achieved\n",
        "best_precision = grid_result.best_score_\n",
        "print(\"Best Precision: {:.2f}%\".format(best_precision * 100))\n",
        "```\n",
        "\n",
        "d. To add Batch Normalization to the model and compare the learning curves, you can modify the model architecture as follows:\n",
        "\n",
        "```python\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n",
        "After making this change, retrain the model using the same code provided in part (a) to compare the learning curves.\n",
        "\n",
        "e. To address potential overfitting, you can add dropout regularization to every layer. Modify the model architecture as follows:\n",
        "\n",
        "```python\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n",
        "Then, retrain the model using the same code provided in part (a) to see if dropout helps in reducing overfitting."
      ],
      "metadata": {
        "id": "1m2OGP3AXJb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Transfer learning.\n",
        "a. Create a new DNN that reuses all the pretrained hidden layers of the previous\n",
        "model, freezes them, and replaces the softmax output layer with a new one.\n",
        "b. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how\n",
        "long it takes. Despite this small number of examples, can you achieve high precision?\n",
        "c. Try caching the frozen layers, and train the model again: how much faster is it now?\n",
        "d. Try again reusing just four hidden layers instead of five. Can you achieve a higher\n",
        "precision?\n",
        "e. Now unfreeze the top two hidden layers and continue training: can you get the\n",
        "model to perform even better?\n",
        "\n",
        "\n",
        "a. To create a new DNN that reuses the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a new one, you can follow these steps:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Filter the dataset to include only digits 5 to 9\n",
        "train_mask = train_labels >= 5\n",
        "test_mask = test_labels >= 5\n",
        "train_images = train_images[train_mask]\n",
        "train_labels = train_labels[train_mask]\n",
        "test_images = test_images[test_mask]\n",
        "test_labels = test_labels[test_mask]\n",
        "\n",
        "# Preprocess the data\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Load the pretrained model\n",
        "pretrained_model = keras.models.load_model('final_model.h5')\n",
        "\n",
        "# Freeze the pretrained layers\n",
        "for layer in pretrained_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new output layer\n",
        "output_layer = keras.layers.Dense(5, activation='softmax')\n",
        "\n",
        "# Create the new model with the pretrained layers and new output layer\n",
        "new_model = keras.Sequential([\n",
        "    pretrained_model,\n",
        "    output_layer\n",
        "])\n",
        "\n",
        "# Compile the new model\n",
        "new_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "b. To train the new DNN on digits 5 to 9 using only 100 images per digit and measure the training time, you can use the following code:\n",
        "\n",
        "```python\n",
        "import time\n",
        "\n",
        "# Reduce the number of examples per digit to 100\n",
        "num_examples_per_digit = 100\n",
        "train_indices = np.arange(len(train_labels))\n",
        "np.random.shuffle(train_indices)\n",
        "train_indices = train_indices[:5*num_examples_per_digit]\n",
        "train_images = train_images[train_indices]\n",
        "train_labels = train_labels[train_indices]\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the new model\n",
        "new_model.fit(train_images, train_labels, epochs=100, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Calculate the training time\n",
        "training_time = time.time() - start_time\n",
        "print(\"Training time: {:.2f} seconds\".format(training_time))\n",
        "```\n",
        "\n",
        "Achieving high precision with only 100 examples per digit might be challenging, but transfer learning can help leverage the knowledge gained from the previous model.\n",
        "\n",
        "c. To cache the frozen layers and train the model again, you can modify the code as follows:\n",
        "\n",
        "```python\n",
        "# Cache the frozen layers\n",
        "for layer in pretrained_model.layers:\n",
        "    layer.trainable = False\n",
        "    layer._name += '_cached'\n",
        "\n",
        "# Compile the new model\n",
        "new_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the new model\n",
        "new_model.fit(train_images, train_labels, epochs=100, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Calculate the training time\n",
        "training_time = time.time() - start_time\n",
        "print(\"Training time with cached frozen layers: {:.2f} seconds\".format(training_time))\n",
        "```\n",
        "\n",
        "Caching the frozen layers can speed up training since the computations for these layers are not repeated during each epoch.\n",
        "\n",
        "d. To reuse just four hidden layers instead of five and aim for higher precision, you can modify the code as follows:\n",
        "\n",
        "```python\n",
        "# Load the pretrained model\n",
        "pretrained_model = keras.models.load_model('final_model.h5')\n",
        "\n",
        "# Freeze the last four pretrained layers\n",
        "for layer in pretrained_model.layers[:-4]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new output layer\n",
        "output_layer = keras.layers.Dense(5, activation='softmax')\n",
        "\n",
        "#\n",
        "\n",
        " Create the new model with four pretrained layers and the new output layer\n",
        "new_model = keras.Sequential([\n",
        "    pretrained_model.layers[0],\n",
        "    pretrained_model.layers[1],\n",
        "    pretrained_model.layers[2],\n",
        "    pretrained_model.layers[3],\n",
        "    output_layer\n",
        "])\n",
        "\n",
        "# Compile the new model\n",
        "new_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the new model\n",
        "new_model.fit(train_images, train_labels, epochs=100, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Calculate the training time\n",
        "training_time = time.time() - start_time\n",
        "print(\"Training time with four hidden layers: {:.2f} seconds\".format(training_time))\n",
        "```\n",
        "\n",
        "By reusing fewer hidden layers, you reduce the complexity of the model and may achieve better precision due to better generalization.\n",
        "\n",
        "e. To unfreeze the top two hidden layers and continue training to further improve performance, you can modify the code as follows:\n",
        "\n",
        "```python\n",
        "# Unfreeze the top two pretrained layers\n",
        "for layer in pretrained_model.layers[-2:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile the new model\n",
        "new_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the new model\n",
        "new_model.fit(train_images, train_labels, epochs=100, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Calculate the training time\n",
        "training_time = time.time() - start_time\n",
        "print(\"Training time with unfrozen top two layers: {:.2f} seconds\".format(training_time))\n",
        "```\n",
        "\n",
        "By unfreezing the top two hidden layers, the model can fine-tune these layers specifically for the new task, potentially improving the overall performance."
      ],
      "metadata": {
        "id": "uqYvwnLjYcFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Pretraining on an auxiliary task.\n",
        "a. In this exercise you will build a DNN that compares two MNIST digit images and\n",
        "predicts whether they represent the same digit or not. Then you will reuse the lower\n",
        "layers of this network to train an MNIST classifier using very little training data. Start\n",
        "by building two DNNs (let’s call them DNN A and B), both similar to the one you built\n",
        "earlier but without the output layer: each DNN should have five hidden layers of 100\n",
        "neurons each, He initialization, and ELU activation. Next, add one more hidden layer\n",
        "with 10 units on top of both DNNs. To do this, you should use\n",
        "TensorFlow’s concat() function with axis=1 to concatenate the outputs of both DNNs\n",
        "for each instance, then feed the result to the hidden layer. Finally, add an output\n",
        "layer with a single neuron using the logistic activation function.\n",
        "b. Split the MNIST training set in two sets: split #1 should containing 55,000 images,\n",
        "and split #2 should contain contain 5,000 images. Create a function that generates a\n",
        "training batch where each instance is a pair of MNIST images picked from split #1.\n",
        "Half of the training instances should be pairs of images that belong to the same\n",
        "class, while the other half should be images from different classes. For each pair, the\n",
        "\n",
        "training label should be 0 if the images are from the same class, or 1 if they are from\n",
        "different classes.\n",
        "c. Train the DNN on this training set. For each image pair, you can simultaneously feed\n",
        "the first image to DNN A and the second image to DNN B. The whole network will\n",
        "gradually learn to tell whether two images belong to the same class or not.\n",
        "d. Now create a new DNN by reusing and freezing the hidden layers of DNN A and\n",
        "adding a softmax output layer on top with 10 neurons. Train this network on split #2\n",
        "and see if you can achieve high performance despite having only 500 images per\n",
        "class.\n",
        "\n",
        "\n",
        "a. To build two DNNs (DNN A and B) for comparing MNIST digit images, and then reuse their lower layers for training an MNIST classifier, you can follow these steps:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Define the DNN architecture for comparing MNIST digit images\n",
        "def create_comparison_model():\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Flatten(input_shape=(28, 28)),\n",
        "        keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "        keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "        keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "        keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "        keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create DNN A\n",
        "dnn_a = create_comparison_model()\n",
        "\n",
        "# Create DNN B\n",
        "dnn_b = create_comparison_model()\n",
        "\n",
        "# Concatenate the outputs of DNN A and DNN B\n",
        "concatenated_output = tf.concat([dnn_a.output, dnn_b.output], axis=1)\n",
        "\n",
        "# Add a hidden layer on top of the concatenated output\n",
        "hidden_layer = keras.layers.Dense(10, activation='elu', kernel_initializer='he_normal')(concatenated_output)\n",
        "\n",
        "# Add the output layer with logistic activation\n",
        "output_layer = keras.layers.Dense(1, activation='sigmoid')(hidden_layer)\n",
        "\n",
        "# Create the final comparison model\n",
        "comparison_model = keras.Model(inputs=[dnn_a.input, dnn_b.input], outputs=output_layer)\n",
        "```\n",
        "\n",
        "b. To generate a training batch where each instance is a pair of MNIST images with corresponding labels indicating whether they belong to the same class or not, you can use the following function:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def generate_training_batch(images, labels, batch_size=32):\n",
        "    num_classes = len(np.unique(labels))\n",
        "    half_batch_size = batch_size // 2\n",
        "\n",
        "    # Select half of the batch as same-class pairs\n",
        "    indices_same_class = np.random.choice(np.where(labels < num_classes)[0], size=half_batch_size, replace=False)\n",
        "    images_same_class = images[indices_same_class]\n",
        "    labels_same_class = np.zeros(half_batch_size)\n",
        "\n",
        "    # Select half of the batch as different-class pairs\n",
        "    indices_diff_class_1 = np.random.choice(np.where(labels < num_classes)[0], size=half_batch_size, replace=False)\n",
        "    indices_diff_class_2 = np.random.choice(np.where(labels >= num_classes)[0], size=half_batch_size, replace=False)\n",
        "    images_diff_class_1 = images[indices_diff_class_1]\n",
        "    images_diff_class_2 = images[indices_diff_class_2]\n",
        "    labels_diff_class = np.ones(half_batch_size)\n",
        "\n",
        "    # Concatenate same-class and different-class pairs\n",
        "    image_pairs = np.concatenate([images_same_class, images_diff_class_1, images_diff_class_2], axis=0)\n",
        "    labels_pairs = np.concatenate([labels_same_class, labels_diff_class], axis=0)\n",
        "\n",
        "    # Shuffle the pairs\n",
        "    shuffle_indices = np.random.permutation(batch_size)\n",
        "    image_pairs = image_pairs[shuffle_indices]\n",
        "    labels_pairs = labels_pairs[shuffle_indices]\n",
        "\n",
        "    return image_pairs, labels_pairs\n",
        "```\n",
        "\n",
        "c. To train the comparison model on the generated training set, you can use the following code:\n",
        "\n",
        "```python\n",
        "# Split the MNIST training set into split #1 and split #2\n",
        "split_1_size = 55000\n",
        "split_2_size = 5000\n",
        "split_1_images = train_images[:split_1_size]\n",
        "split_1_labels = train\n",
        "\n",
        "_labels[:split_1_size]\n",
        "split_2_images = train_images[split_1_size:split_1_size + split_2_size]\n",
        "split_2_labels = train_labels[split_1_size:split_1_size + split_2_size]\n",
        "\n",
        "# Compile the comparison model\n",
        "comparison_model.compile(optimizer='adam',\n",
        "                         loss='binary_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "\n",
        "# Train the comparison model\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "num_steps_per_epoch = len(split_1_images) // batch_size\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for step in range(num_steps_per_epoch):\n",
        "        image_pairs, labels_pairs = generate_training_batch(split_1_images, split_1_labels, batch_size)\n",
        "        comparison_model.train_on_batch([image_pairs[:, 0], image_pairs[:, 1]], labels_pairs)\n",
        "```\n",
        "\n",
        "d. To create a new DNN by reusing and freezing the hidden layers of DNN A and adding a softmax output layer on top for training on split #2, you can use the following code:\n",
        "\n",
        "```python\n",
        "# Freeze the hidden layers of DNN A\n",
        "for layer in dnn_a.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add a softmax output layer on top of DNN A\n",
        "output_layer = keras.layers.Dense(10, activation='softmax')(dnn_a.output)\n",
        "\n",
        "# Create the final classifier model\n",
        "classifier_model = keras.Model(inputs=dnn_a.input, outputs=output_layer)\n",
        "\n",
        "# Compile the classifier model\n",
        "classifier_model.compile(optimizer='adam',\n",
        "                         loss='sparse_categorical_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "\n",
        "# Train the classifier model on split #2\n",
        "classifier_model.fit(split_2_images, split_2_labels, epochs=10, validation_data=(test_images, test_labels))\n",
        "```\n",
        "\n",
        "Even with only 500 images per class, transfer learning by reusing the lower layers of DNN A can help achieve high performance on split #2."
      ],
      "metadata": {
        "id": "P_5GGmAbaVJK"
      }
    }
  ]
}