{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Write the Python code to implement a single neuron.\n",
        "\n",
        "Certainly! Here's an example of Python code to implement a single neuron:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "class SingleNeuron:\n",
        "    def __init__(self, input_size):\n",
        "        self.weights = np.random.randn(input_size)  # Initialize weights randomly\n",
        "        self.bias = np.random.randn()  # Initialize bias randomly\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Perform the weighted sum of inputs and add the bias\n",
        "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
        "        output = self.activation(weighted_sum)  # Apply activation function\n",
        "        return output\n",
        "\n",
        "    def activation(self, x):\n",
        "        # Use sigmoid activation function\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "```\n",
        "\n",
        "In this code, we define a `SingleNeuron` class that represents a single neuron. The constructor initializes the neuron's weights and bias with random values. The `forward` method takes an input array, performs the weighted sum of inputs, adds the bias, and applies the sigmoid activation function to produce the output.\n",
        "\n",
        "You can use this code to create an instance of the `SingleNeuron` class and feed inputs to the neuron by calling the `forward` method. For example:\n",
        "\n",
        "```python\n",
        "# Create a single neuron with input size 3\n",
        "neuron = SingleNeuron(input_size=3)\n",
        "\n",
        "# Provide input values\n",
        "inputs = np.array([0.5, 0.2, 0.8])\n",
        "\n",
        "# Get the output of the neuron\n",
        "output = neuron.forward(inputs)\n",
        "\n",
        "print(\"Output:\", output)\n",
        "```\n",
        "\n",
        "This code snippet creates a single neuron with an input size of 3. It provides an input array with three values (0.5, 0.2, 0.8) and computes the output of the neuron using the `forward` method. The resulting output is then printed to the console."
      ],
      "metadata": {
        "id": "UZNT-VzBOXAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Write the Python code to implement ReLU.\n",
        "\n",
        "Certainly! Here's an example of Python code to implement the Rectified Linear Unit (ReLU) activation function:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "```\n",
        "\n",
        "In this code, we define a function `relu` that takes an input `x` and applies the ReLU activation function element-wise using the `np.maximum` function from the NumPy library. The ReLU function returns the maximum value between 0 and `x`, effectively replacing any negative values with zeros.\n",
        "\n",
        "You can use this code by calling the `relu` function and passing in the desired input value or an array of values. For example:\n",
        "\n",
        "```python\n",
        "# Test ReLU activation function\n",
        "x = np.array([-2.5, 1.2, 0, -4.7, 3.5])\n",
        "\n",
        "# Apply ReLU activation to the input\n",
        "output = relu(x)\n",
        "\n",
        "print(\"Output:\", output)\n",
        "```\n",
        "\n",
        "In this example, we provide an input array `x` with five values. We apply the ReLU activation function to the input array by calling the `relu` function, and the resulting output is printed to the console. The ReLU function sets negative values to zero while leaving positive and zero values unchanged."
      ],
      "metadata": {
        "id": "5JEUbnpzOi6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Write the Python code for a dense layer in terms of matrix multiplication.\n",
        "\n",
        "Certainly! Here's an example of Python code to implement a dense layer using matrix multiplication:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "class DenseLayer:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.randn(input_size, output_size)  # Initialize weights randomly\n",
        "        self.bias = np.random.randn(output_size)  # Initialize bias randomly\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Perform matrix multiplication of inputs and weights, and add the bias\n",
        "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
        "        return weighted_sum\n",
        "```\n",
        "\n",
        "In this code, we define a `DenseLayer` class that represents a dense layer in a neural network. The constructor takes two arguments: `input_size` and `output_size`, which specify the dimensions of the weight matrix. The constructor initializes the weights and bias with random values.\n",
        "\n",
        "The `forward` method takes an input array and performs the matrix multiplication of the inputs and weights using the `np.dot` function. It then adds the bias term to the result and returns the weighted sum.\n",
        "\n",
        "You can use this code to create an instance of the `DenseLayer` class and feed inputs to the dense layer by calling the `forward` method. For example:\n",
        "\n",
        "```python\n",
        "# Create a dense layer with input size 3 and output size 2\n",
        "dense_layer = DenseLayer(input_size=3, output_size=2)\n",
        "\n",
        "# Provide input values\n",
        "inputs = np.array([0.5, 0.2, 0.8])\n",
        "\n",
        "# Get the output of the dense layer\n",
        "output = dense_layer.forward(inputs)\n",
        "\n",
        "print(\"Output:\", output)\n",
        "```\n",
        "\n",
        "In this code snippet, we create a dense layer with an input size of 3 and an output size of 2. We provide an input array with three values (0.5, 0.2, 0.8) and compute the output of the dense layer using the `forward` method. The resulting output, which is the weighted sum of the inputs, is then printed to the console."
      ],
      "metadata": {
        "id": "Tz4DJldaOs74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Write the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python).\n",
        "\n",
        "Certainly! Here's an example of Python code to implement a dense layer using plain Python with list comprehensions:\n",
        "\n",
        "```python\n",
        "import random\n",
        "\n",
        "class DenseLayer:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = [[random.random() for _ in range(input_size)] for _ in range(output_size)]\n",
        "        self.bias = [random.random() for _ in range(output_size)]\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Perform matrix multiplication of inputs and weights, and add the bias\n",
        "        weighted_sum = [\n",
        "            sum(input_val * weight_val for input_val, weight_val in zip(inputs, weight_row)) + bias_val\n",
        "            for weight_row, bias_val in zip(self.weights, self.bias)\n",
        "        ]\n",
        "        return weighted_sum\n",
        "```\n",
        "\n",
        "In this code, we define a `DenseLayer` class that represents a dense layer in a neural network. The constructor takes two arguments: `input_size` and `output_size`, which specify the dimensions of the weight matrix. The constructor initializes the weights and bias using list comprehensions and random values generated by the `random.random()` function.\n",
        "\n",
        "The `forward` method takes an input list and performs the matrix multiplication of the inputs and weights using list comprehensions. It iterates over each row of weights and calculates the dot product between the input values and the corresponding row of weights. It then adds the bias term to each element of the resulting list to obtain the weighted sum.\n",
        "\n",
        "You can use this code to create an instance of the `DenseLayer` class and feed inputs to the dense layer by calling the `forward` method. For example:\n",
        "\n",
        "```python\n",
        "# Create a dense layer with input size 3 and output size 2\n",
        "dense_layer = DenseLayer(input_size=3, output_size=2)\n",
        "\n",
        "# Provide input values\n",
        "inputs = [0.5, 0.2, 0.8]\n",
        "\n",
        "# Get the output of the dense layer\n",
        "output = dense_layer.forward(inputs)\n",
        "\n",
        "print(\"Output:\", output)\n",
        "```\n",
        "\n",
        "In this code snippet, we create a dense layer with an input size of 3 and an output size of 2. We provide an input list with three values [0.5, 0.2, 0.8] and compute the output of the dense layer using the `forward` method. The resulting output, which is the weighted sum of the inputs, is then printed to the console."
      ],
      "metadata": {
        "id": "0ubLiokfO6xs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. What is the “hidden size” of a layer?\n",
        "\n",
        "The \"hidden size\" of a layer refers to the number of neurons or units present in that layer. It represents the dimensionality or the number of internal representations or features that the layer can learn. Each neuron in the hidden layer takes input from the previous layer and applies weights to those inputs, followed by an activation function, to produce an output.\n",
        "\n",
        "In a neural network, each layer consists of one or more hidden layers sandwiched between an input layer and an output layer. The hidden layers play a crucial role in learning complex patterns and representations from the input data. The hidden size determines the capacity and complexity of the layer, as well as the expressive power of the neural network.\n",
        "\n",
        "The hidden size is typically a hyperparameter that needs to be determined during the model design phase. It depends on factors such as the complexity of the task, the amount of available training data, and the trade-off between model capacity and computational resources. Increasing the hidden size can potentially allow the layer to learn more intricate and nuanced representations but may also lead to overfitting or increased computational requirements. On the other hand, reducing the hidden size may simplify the model and make it more computationally efficient, but it could result in a loss of expressive power.\n",
        "\n",
        "Determining an appropriate hidden size often involves experimentation and model validation. It is common to start with a small hidden size and gradually increase it until the model's performance reaches a satisfactory level or until computational constraints are met. Techniques such as cross-validation and model evaluation on a held-out validation set can help in selecting an optimal hidden size for a given task."
      ],
      "metadata": {
        "id": "IyeMyr8_QjHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. What does the t method do in PyTorch?\n",
        "\n",
        "In PyTorch, the `t` method is used to transpose a tensor. Transposing a tensor swaps its dimensions, effectively rearranging the elements along the specified axes.\n",
        "\n",
        "The `t` method can be called on a PyTorch tensor object, and it returns a new tensor that is the transposed version of the original tensor. It is primarily used for operations such as matrix transposition or changing the shape of a tensor.\n",
        "\n",
        "Here's an example to illustrate the usage of the `t` method:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Create a tensor\n",
        "x = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "\n",
        "# Transpose the tensor\n",
        "y = x.t()\n",
        "\n",
        "print(\"Original tensor:\")\n",
        "print(x)\n",
        "\n",
        "print(\"\\nTransposed tensor:\")\n",
        "print(y)\n",
        "```\n",
        "\n",
        "Output:\n",
        "```\n",
        "Original tensor:\n",
        "tensor([[1, 2, 3],\n",
        "        [4, 5, 6]])\n",
        "\n",
        "Transposed tensor:\n",
        "tensor([[1, 4],\n",
        "        [2, 5],\n",
        "        [3, 6]])\n",
        "```\n",
        "\n",
        "In the above example, we create a 2D tensor `x` with shape (2, 3). We then call the `t` method on `x` to obtain the transposed tensor `y`. The resulting transposed tensor `y` has a shape of (3, 2), where the elements have been rearranged along the corresponding axes.\n",
        "\n",
        "Note that the `t` method performs a shallow transpose, which means it does not actually move or copy the underlying data but rather adjusts the strides of the tensor. If you need a deep copy of the transposed tensor, you can use the `transpose` method with appropriate arguments or the `torch.transpose` function."
      ],
      "metadata": {
        "id": "TE7KJo6-Qx-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Why is matrix multiplication written in plain Python very slow?\n",
        "\n",
        "Matrix multiplication implemented in plain Python can be slow compared to optimized implementations in libraries like NumPy or TensorFlow for several reasons:\n",
        "\n",
        "1. Lack of vectorization: In plain Python, matrix multiplication is typically implemented using nested loops, which can be slow due to the overhead of interpreted code execution. It doesn't take advantage of efficient low-level operations or hardware optimizations like SIMD (Single Instruction, Multiple Data) instructions that can be utilized by optimized libraries.\n",
        "\n",
        "2. Python's Global Interpreter Lock (GIL): The GIL in CPython, the default Python interpreter, prevents true multi-threading, which can limit the speedup achievable through parallelization. This can impact performance when performing large-scale matrix operations that could otherwise benefit from parallel execution.\n",
        "\n",
        "3. Dynamic typing and function call overhead: Python's dynamic typing and function call overhead can add additional computational and memory overhead when performing matrix multiplication. Type checking and function call dispatching incur extra cost compared to statically typed compiled languages.\n",
        "\n",
        "4. Lack of low-level optimizations: Libraries like NumPy and TensorFlow are implemented using efficient low-level code, often written in C or C++, and leverage optimized linear algebra libraries like BLAS (Basic Linear Algebra Subprograms) or MKL (Math Kernel Library). These libraries are highly optimized for matrix operations and take advantage of hardware-specific optimizations.\n",
        "\n",
        "By using optimized libraries like NumPy or TensorFlow, you can take advantage of their highly optimized implementations, efficient memory management, parallel execution, and low-level optimizations. These libraries leverage efficient algorithms and utilize optimized linear algebra libraries, resulting in significantly faster matrix multiplication compared to plain Python implementations."
      ],
      "metadata": {
        "id": "cYl3bWmfQ9NR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. In matmul, why is ac==br?\n",
        "\n",
        "In matrix multiplication (matmul), the condition `ac == br` is required for the matrices to be compatible for multiplication. Here's what each term represents:\n",
        "\n",
        "- `a` is the left matrix with dimensions `(a_rows, a_cols)`.\n",
        "- `b` is the right matrix with dimensions `(b_rows, b_cols)`.\n",
        "- `c` is the resulting matrix with dimensions `(a_rows, b_cols)`.\n",
        "\n",
        "To perform matrix multiplication, the number of columns in the left matrix (`a_cols`) must be equal to the number of rows in the right matrix (`b_rows`). Mathematically, this can be expressed as `a_cols == b_rows`.\n",
        "\n",
        "In the context of `ac == br`, it means the number of columns in the left matrix (`a_cols`) is equal to the number of rows in the right matrix (`b_rows`). This condition ensures that the dimensions align properly for matrix multiplication.\n",
        "\n",
        "If the condition `ac == br` is not satisfied, it is not possible to perform matrix multiplication between the given matrices, and you will encounter a dimension mismatch error."
      ],
      "metadata": {
        "id": "Pahr0D2PRKw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
        "\n",
        "In Jupyter Notebook, you can measure the time taken for a single cell to execute using the `%timeit` magic command or the `%%timeit` cell magic command. These commands provide timing information and can be used to measure the execution time of a single line of code or an entire cell.\n",
        "\n",
        "To measure the time taken for a single line of code, you can use `%timeit` followed by the line of code. For example:\n",
        "\n",
        "```python\n",
        "%timeit my_function()\n",
        "```\n",
        "\n",
        "This will run `my_function()` multiple times and provide information about the average execution time.\n",
        "\n",
        "To measure the time taken for an entire cell to execute, you can use `%%timeit` at the beginning of the cell. For example:\n",
        "\n",
        "```python\n",
        "%%timeit\n",
        "\n",
        "# Code in this cell\n",
        "statement1()\n",
        "statement2()\n",
        "```\n",
        "\n",
        "When you run the cell with `%%timeit`, it will execute the code within the cell multiple times and provide timing information, including the average execution time.\n",
        "\n",
        "Additionally, if you only want to measure the time taken for a single execution of a cell (rather than an average), you can use the `%%time` cell magic command instead of `%%timeit`.\n",
        "\n",
        "Both `%timeit` and `%%timeit` provide useful timing information that can help you measure the performance of your code and identify potential bottlenecks."
      ],
      "metadata": {
        "id": "n0l3oMxoRW2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. What is elementwise arithmetic?\n",
        "\n",
        "Elementwise arithmetic, also known as elementwise operations, refers to performing arithmetic operations on corresponding elements of two or more arrays or tensors. In elementwise arithmetic, the operation is applied individually to each element without considering the relationship between elements in different arrays.\n",
        "\n",
        "For example, consider two arrays `A` and `B`:\n",
        "\n",
        "```python\n",
        "A = [1, 2, 3, 4]\n",
        "B = [5, 6, 7, 8]\n",
        "```\n",
        "\n",
        "Elementwise addition of `A` and `B` would result in a new array `C`:\n",
        "\n",
        "```python\n",
        "C = A + B = [6, 8, 10, 12]\n",
        "```\n",
        "\n",
        "Each element of `C` is obtained by adding the corresponding elements from `A` and `B`.\n",
        "\n",
        "Similarly, elementwise subtraction, multiplication, and division can be performed between arrays or tensors of the same shape. The operation is applied element by element, resulting in a new array or tensor with the same shape as the input.\n",
        "\n",
        "Elementwise arithmetic operations are commonly used in numerical computations, scientific computing, and deep learning. They provide a convenient and efficient way to perform operations on arrays or tensors without the need for explicit loops. Libraries like NumPy and TensorFlow provide optimized implementations for efficient elementwise arithmetic operations on multi-dimensional arrays or tensors."
      ],
      "metadata": {
        "id": "QV-LNqTMSEWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Write the PyTorch code to test whether every element of a is greater than the corresponding element of b.\n",
        "\n",
        "To test whether every element of `a` is greater than the corresponding element of `b` in PyTorch, you can use the `torch.all` function along with a comparison operator. Here's an example code snippet:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "a = torch.tensor([2, 4, 6, 8])\n",
        "b = torch.tensor([1, 3, 5, 7])\n",
        "\n",
        "result = torch.all(a > b)\n",
        "\n",
        "print(result)\n",
        "```\n",
        "\n",
        "Output:\n",
        "```\n",
        "tensor(True)\n",
        "```\n",
        "\n",
        "In the above code, we create two PyTorch tensors `a` and `b` with the same shape. We then compare each element of `a` with the corresponding element of `b` using the `>` operator, which returns a boolean tensor indicating whether the condition is true for each element.\n",
        "\n",
        "The `torch.all` function is used to check if every element of the resulting boolean tensor is `True`. It returns a single boolean value indicating whether the condition is true for all elements.\n",
        "\n",
        "In the given example, since every element of `a` is greater than the corresponding element of `b`, the result will be `True`. If any element fails the condition, the result will be `False`."
      ],
      "metadata": {
        "id": "HBV5fvedSTfU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. What is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
        "\n",
        "In PyTorch, a rank-0 tensor refers to a tensor with no dimensions or shape. It represents a single scalar value. A rank-0 tensor is often referred to as a \"0-dimensional tensor\" or a \"scalar tensor\".\n",
        "\n",
        "To convert a rank-0 tensor to a plain Python data type, you can use the `.item()` method. This method extracts the scalar value from the tensor and returns it as a Python native type.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "x = torch.tensor(5)  # Rank-0 tensor (scalar)\n",
        "\n",
        "# Convert the rank-0 tensor to a Python data type\n",
        "value = x.item()\n",
        "\n",
        "print(value)\n",
        "print(type(value))\n",
        "```\n",
        "\n",
        "Output:\n",
        "```\n",
        "5\n",
        "<class 'int'>\n",
        "```\n",
        "\n",
        "In the above code, we create a rank-0 tensor `x` with the value `5`. By calling the `.item()` method on `x`, we extract the scalar value and assign it to the variable `value`. The `value` variable now contains a plain Python integer (`int`) type.\n",
        "\n",
        "The `.item()` method can be used with rank-0 tensors containing other data types like floats as well. However, it's important to note that if the tensor has more than one element (non-scalar), calling `.item()` will raise an error since it is only applicable to rank-0 tensors."
      ],
      "metadata": {
        "id": "dGgPX29ES9s3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. How does elementwise arithmetic help us speed up matmul?\n",
        "\n",
        "Elementwise arithmetic operations do not directly speed up the matrix multiplication (`matmul`) operation itself. In fact, elementwise arithmetic and matrix multiplication are distinct operations with different computational complexities.\n",
        "\n",
        "Elementwise arithmetic involves performing arithmetic operations on corresponding elements of arrays or tensors individually. It operates on each element independently and does not take into account the relationships between elements in different arrays.\n",
        "\n",
        "On the other hand, matrix multiplication is a more complex operation that involves multiplying and summing elements from different matrices according to specific rules. It combines elements from different positions to compute new values for each element in the resulting matrix.\n",
        "\n",
        "While elementwise arithmetic does not directly speed up matrix multiplication, it can be useful in accelerating certain computations involving matrices. Here are a few scenarios where elementwise arithmetic can be beneficial:\n",
        "\n",
        "1. Broadcasting: Elementwise operations can take advantage of broadcasting, which allows arrays or tensors of different shapes to be combined in elementwise operations without explicitly duplicating the smaller array. Broadcasting can help simplify and optimize computations that involve matrix multiplication along with other elementwise operations.\n",
        "\n",
        "2. Pre- or post-processing: Elementwise arithmetic operations can be performed as preprocessing or post-processing steps in a larger computation pipeline that involves matrix multiplication. These operations can manipulate the elements of the matrices or transform the results to meet specific requirements or constraints.\n",
        "\n",
        "3. Elementwise activation functions: Activation functions such as ReLU, sigmoid, or softmax are applied elementwise to the results of matrix multiplication in neural networks. These activation functions introduce non-linearities to the network, enabling it to learn complex relationships between features.\n",
        "\n",
        "In summary, while elementwise arithmetic does not directly speed up matrix multiplication, it can play a supporting role in certain computations involving matrices, such as broadcasting, preprocessing, post-processing, or applying activation functions in neural networks. It helps in manipulating and transforming the elements of matrices to facilitate efficient and effective computations."
      ],
      "metadata": {
        "id": "rFDMGtVMVVYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. What are the broadcasting rules?\n",
        "\n",
        "Broadcasting is a mechanism in NumPy and PyTorch that allows arrays of different shapes to be combined in elementwise operations without explicitly duplicating the smaller array. The broadcasting rules define how arrays with different shapes can be aligned and operated upon. The rules are as follows:\n",
        "\n",
        "1. Rule 1: If the two arrays have different numbers of dimensions, the array with fewer dimensions is padded with ones on its left side until the number of dimensions matches.\n",
        "\n",
        "2. Rule 2: If the shape of any dimension of the two arrays does not match, the array with the shape equal to 1 in that dimension is stretched or replicated along that dimension to match the shape of the other array.\n",
        "\n",
        "3. Rule 3: If the shape of any dimension of the two arrays is neither equal to 1 nor equal to the other shape, a broadcasting error occurs, indicating that the shapes are incompatible for elementwise operations.\n",
        "\n",
        "Here's an example to illustrate the broadcasting rules:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([[1, 2, 3], [4, 5, 6]])  # Shape: (2, 3)\n",
        "B = np.array([10, 20, 30])  # Shape: (3,)\n",
        "\n",
        "# Broadcasting: B is stretched along the first dimension to match the shape of A\n",
        "result = A + B\n",
        "\n",
        "print(result)\n",
        "```\n",
        "\n",
        "Output:\n",
        "```\n",
        "[[11 22 33]\n",
        " [14 25 36]]\n",
        "```\n",
        "\n",
        "In the example, we have an array `A` with shape `(2, 3)` and an array `B` with shape `(3,)`. According to the broadcasting rules, the second array `B` is stretched along the first dimension to match the shape of `A`. The resulting operation performs elementwise addition between `A` and the stretched `B`, resulting in the output `[[11, 22, 33], [14, 25, 36]]`.\n",
        "\n",
        "Broadcasting allows for more compact and efficient code by avoiding the need to explicitly duplicate arrays to match shapes before performing elementwise operations. It simplifies computations and improves code readability, particularly in scenarios involving arrays of different shapes or dimensions."
      ],
      "metadata": {
        "id": "7pTOwr_3VyE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. What is expand_as? Show an example of how it can be used to match the results of broadcasting.\n",
        "\n",
        "In PyTorch, the `expand_as` method is used to expand the size of a tensor to match the size of another tensor. It is a convenient way to ensure that two tensors have the same shape for elementwise operations or broadcasting.\n",
        "\n",
        "Here's an example that demonstrates how `expand_as` can be used to match the results of broadcasting:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "A = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Shape: (2, 3)\n",
        "B = torch.tensor([10, 20, 30])  # Shape: (3,)\n",
        "\n",
        "# Expand tensor B to match the shape of A\n",
        "expanded_B = B.expand_as(A)\n",
        "\n",
        "# Perform elementwise addition between A and expanded_B\n",
        "result = A + expanded_B\n",
        "\n",
        "print(result)\n",
        "```\n",
        "\n",
        "Output:\n",
        "```\n",
        "tensor([[11, 22, 33],\n",
        "        [14, 25, 36]])\n",
        "```\n",
        "\n",
        "In the example, we have a tensor `A` with shape `(2, 3)` and a tensor `B` with shape `(3,)`. To match the shape of `A`, we use the `expand_as` method on `B` to expand its size. The `expand_as(A)` call expands the tensor `B` to the shape `(2, 3)`, making it compatible for elementwise operations with `A`.\n",
        "\n",
        "The expanded tensor `expanded_B` now has the same shape as `A`. We then perform elementwise addition between `A` and `expanded_B`, resulting in the output tensor `[[11, 22, 33], [14, 25, 36]]`.\n",
        "\n",
        "Using `expand_as` eliminates the need to manually reshape or broadcast tensors, ensuring they have the same shape for elementwise operations or broadcasting. It simplifies code and promotes more concise and readable computations."
      ],
      "metadata": {
        "id": "6_mpWlmwWEt5"
      }
    }
  ]
}